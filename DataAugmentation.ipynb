{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB,dest='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/imdb/test'),\n",
       " PosixPath('data/imdb/README'),\n",
       " PosixPath('data/imdb/unsup'),\n",
       " PosixPath('data/imdb/imdb.vocab'),\n",
       " PosixPath('data/imdb/tmp_lm'),\n",
       " PosixPath('data/imdb/train'),\n",
       " PosixPath('data/imdb/tmp_clas')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_folder(path,test='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>new . xxmaj have we all forgotten xxmaj de xxmaj sica 's \" xxmaj bicycle xxmaj thief \" ? xxmaj or anything by xxmaj hitchcock ? \\n \\n  xxmaj so all we get is the extended metaphor of the juvenile as xxmaj truffaut , who spends all his free time ' screwing up his eyes ' at the movies , who wrecks schoolroom discipline , gets accused of xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>film and plays his usual , smarmy , egotistical , snotty character that was actually endearing in \" xxmaj pretty xxmaj in xxmaj pink \" and has xxup not been amusing ever since . xxmaj grating does n't even begin to describe his performance . xxmaj ricky ( xxmaj mark xxmaj xxunk ) is a muscular , blonde , struggling actor who ( gasp ! ) is only worried about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>xxmaj clan , they 're nasty &amp; they do bad things but they ai n't go no soul . i see a lot of reviews from people that liked this , and i guess i do n't know what i missed , but i found it to be very mediocre &amp; i would n't recommend it to anyone , really . 4 out of 10 . xxbos xxmaj this movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>see if you can count the cliches . xxmaj and yes , xxmaj blair gets possessed , as if you did n't see xxup that coming down xxmaj main xxmaj street followed by a marching band . \\n \\n  xxmaj no stars . \" xxmaj witchery \" - these witches will give you xxunk . xxbos xxmaj charlotte 's deadly beauty and lethal kicks make the movie cooler than</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>stallone , who co - wrote the script with xxmaj james xxmaj cameron ( a long way from the exciting xxup terminator made the previous xxunk seems to have given the xxmaj rambo character as little to say in understandable xxmaj english , and merely comes out with moronic grunts , almost as though he has invented his own brand of patois only understandable to himself . xxmaj maybe his</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "import en_core_web_lg\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synalter_Noun_Verb(cand_word,POS):\n",
    "    thresh = -1\n",
    "    scores = []\n",
    "    synonyms = wordnet.synsets(cand_word)\n",
    "    for w2 in synonyms:\n",
    "        try:\n",
    "            w1 = wordnet.synset(cand_word+'.'+POS+'.01') \n",
    "        #w2 = wordnet.synset(word+'.'+POS+'.01') # n denotes noun \n",
    "            if (w1.wup_similarity(w2)>thresh):\n",
    "                scores.append(w1.wup_similarity(w2))\n",
    "            return synonyms[np.argmax(scores)].name().split('.')[0].replace('_',' ')\n",
    "        except:\n",
    "            if not scores:\n",
    "                for synset in synonyms:\n",
    "                    word=synset.name().split('.')[0].replace('_',' ')\n",
    "                    #print(word)\n",
    "                    token = nlp(word)\n",
    "                    token_main = nlp(cand_word)\n",
    "                    if token.vector_norm and token_main.vector_norm and float(token.similarity(token_main)) > thresh:\n",
    "                        scores.append(float(token.similarity(token_main)))\n",
    "            return synonyms[np.argmax(scores)].name().split('.')[0].replace('_',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADJ', 'NOUN', 'VERB']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('bright car running')\n",
    "[tok.pos_ for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_words(word_str,pos,output_text):\n",
    "    change_word=synalter_Noun_Verb(word_str,pos)\n",
    "    if change_word:\n",
    "        search_word = re.search(r'\\b('+word_str+r')\\b', output_text)\n",
    "        Loc = search_word.start()\n",
    "        output_text = output_text[:int(Loc)] + change_word + output_text[int(Loc) + len(word_str):] \n",
    "        \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100006"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob(str(path)+'/**/*.txt',recursive=True)\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100002"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [f for f in files if '_aug.txt' not in f] #skip already augmented files\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_augmenter(object):\n",
    "#    def __init__(self):\n",
    "    \n",
    "    def __call__(self, fn,i): \n",
    "        #print(f'augmenting file {fn}')\n",
    "        output_text = open(fn,'r').read()\n",
    "        #print(\"Sentence: \"+text)\n",
    "        counts = Counter(output_text.split())\n",
    "        noun = []\n",
    "        verb = []\n",
    "        doc = nlp(' '.join(w for w in [key for key,value in counts.items() if value ==1]))\n",
    "        noun = [tok.text for tok in doc if tok.pos_ == 'NOUN']\n",
    "        verb = [tok.text for tok in doc if tok.pos_ == 'VERB']\n",
    "        adj = [tok.text for tok in doc if tok.pos_ == 'ADJ']\n",
    "        cand_words = noun+verb+adj\n",
    "        len_all = len(verb+noun+adj)\n",
    "        random.seed(4)\n",
    "        temp = random.sample(range(len_all), random.randint(0,int(len_all*0.7)))\n",
    "        #print([cand_words[i] for i in temp])\n",
    "        for i in temp:\n",
    "            word_str = cand_words[i]\n",
    "            if i<len(verb):output_text = change_words(word_str,'v',output_text)\n",
    "            elif i<len(noun): output_text = change_words(word_str,'n',output_text)\n",
    "            else:output_text=change_words(word_str,'a',output_text)\n",
    "        new_fn = fn.replace('.txt','_aug.txt')\n",
    "        with open(new_fn,'w') as f:\n",
    "            f.write(output_text)\n",
    "        #print(f'saving augmented file to {new_fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parallel(text_augmenter(),files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100336"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob(str(path)+'/**/*.txt',recursive=True)\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
